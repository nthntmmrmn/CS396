{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1. Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression r2 Score: 0.5530311140279232\n",
      "GBTree Regression r2 Score: 0.6698645135097733\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "cal = datasets.fetch_california_housing()\n",
    "X = cal['data']\n",
    "y = cal['target']\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "r2_scores_reg = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    reg = LinearRegression().fit(X_train, y_train)\n",
    "    r2_scores_reg.append(reg.score(X_test, y_test))\n",
    "\n",
    "print(f'Linear Regression r2 Score: {np.mean(r2_scores_reg)}')\n",
    "\n",
    "r2_scores_GBR = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    GBR = GradientBoostingRegressor().fit(X_train, y_train)\n",
    "    r2_scores_GBR.append(GBR.score(X_test, y_test))\n",
    "\n",
    "print(f'GBTree Regression r2 Score: {np.mean(r2_scores_GBR)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.01 \t n_est: 50 \t depth: 1 \t r2: 0.13716778094240847\n",
      "LR: 0.01 \t n_est: 50 \t depth: 3 \t r2: 0.2803357862690098\n",
      "LR: 0.01 \t n_est: 50 \t depth: 5 \t r2: 0.32040091949643623\n",
      "LR: 0.01 \t n_est: 100 \t depth: 1 \t r2: 0.2556720337734954\n",
      "LR: 0.01 \t n_est: 100 \t depth: 3 \t r2: 0.4350438301683216\n",
      "LR: 0.01 \t n_est: 100 \t depth: 5 \t r2: 0.486592980599929\n",
      "LR: 0.01 \t n_est: 200 \t depth: 1 \t r2: 0.3635632700884985\n",
      "LR: 0.01 \t n_est: 200 \t depth: 3 \t r2: 0.5445304370452579\n",
      "LR: 0.01 \t n_est: 200 \t depth: 5 \t r2: 0.5982635276657959\n",
      "LR: 0.1 \t n_est: 50 \t depth: 1 \t r2: 0.46718980922217296\n",
      "LR: 0.1 \t n_est: 50 \t depth: 3 \t r2: 0.6330041985338623\n",
      "LR: 0.1 \t n_est: 50 \t depth: 5 \t r2: 0.6661536552866457\n",
      "LR: 0.1 \t n_est: 100 \t depth: 1 \t r2: 0.5365069651471427\n",
      "LR: 0.1 \t n_est: 100 \t depth: 3 \t r2: 0.6698649765200339\n",
      "LR: 0.1 \t n_est: 100 \t depth: 5 \t r2: 0.6455269342496381\n",
      "LR: 0.1 \t n_est: 200 \t depth: 1 \t r2: 0.5867343590454117\n",
      "LR: 0.1 \t n_est: 200 \t depth: 3 \t r2: 0.6785686943002426\n",
      "LR: 0.1 \t n_est: 200 \t depth: 5 \t r2: 0.6426375176076486\n",
      "LR: 0.5 \t n_est: 50 \t depth: 1 \t r2: 0.5851024032093719\n",
      "LR: 0.5 \t n_est: 50 \t depth: 3 \t r2: 0.6358382512504972\n",
      "LR: 0.5 \t n_est: 50 \t depth: 5 \t r2: 0.6328315118905145\n",
      "LR: 0.5 \t n_est: 100 \t depth: 1 \t r2: 0.6061969370253413\n",
      "LR: 0.5 \t n_est: 100 \t depth: 3 \t r2: 0.6458062421238864\n",
      "LR: 0.5 \t n_est: 100 \t depth: 5 \t r2: 0.6139414990504313\n",
      "LR: 0.5 \t n_est: 200 \t depth: 1 \t r2: 0.6164129421863059\n",
      "LR: 0.5 \t n_est: 200 \t depth: 3 \t r2: 0.6326880480652705\n",
      "LR: 0.5 \t n_est: 200 \t depth: 5 \t r2: 0.5902868528095094\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "cal = datasets.fetch_california_housing()\n",
    "X = cal['data']\n",
    "y = cal['target']\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "learning_rate = [0.01, 0.1, 0.5]\n",
    "n_estimators = [50, 100, 200]\n",
    "max_depth = [1, 3, 5]\n",
    "\n",
    "for l in learning_rate:\n",
    "    for n in n_estimators:\n",
    "        for m in max_depth:\n",
    "            r2_scores = []\n",
    "            GBR = GradientBoostingRegressor(learning_rate=l, n_estimators=n, max_depth=m)\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                GBR.fit(X_train, y_train)\n",
    "                r2_scores.append(GBR.score(X_test, y_test))\n",
    "            print(f'LR: {l} \\t n_est: {n} \\t depth: {m} \\t r2: {np.mean(r2_scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.\n",
    "\n",
    "#### Performance and Conclusions\n",
    "Based on the r2 scores from part (a), Gradient Boosting Tree Regression performs significantly better than Linear Regression (0.66 versus 0.55), and thus it more accurately fits the data.\n",
    "\n",
    "Based on part (b), the Gradient Boosting Tree Regression is more accurate with a medium learning-rate (0.1) than small (0.01) or large (0.5) ones. This is likely because a small learning-rate can often lead to overfitting, whereas a large learning-rate often misses the best-fitting regression line. There is also a tradeoff between the number of n_estimators and depth: if the number of n_est is low, the regressor performs better with a larger depth (5), while if the number of n_est is high, the algorithm performs better with a lower depth (3). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification Accuracy Score: 0.7922965116279069\n",
      "GBTree Classification Accuracy Score: 0.8382267441860465\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cal = datasets.fetch_california_housing()\n",
    "X = cal['data']\n",
    "y = cal['target']\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "accuracy_log = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    log = LogisticRegression(solver='liblinear').fit(X_train, np.where(y_train > 2, 1, 0))\n",
    "    y_pred = log.predict(X_test)\n",
    "    accuracy_log.append(accuracy_score(y_pred, np.where(y_test > 2, 1, 0)))\n",
    "\n",
    "print(f'Logistic Regression Classification Accuracy Score: {np.mean(accuracy_log)}')\n",
    "\n",
    "accuracy_GBC = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    GBC = GradientBoostingClassifier().fit(X_train, np.where(y_train > 2, 1, 0))\n",
    "    y_pred = GBC.predict(X_test)\n",
    "    accuracy_GBC.append(accuracy_score(y_pred, np.where(y_test > 2, 1, 0)))\n",
    "\n",
    "print(f'GBTree Classification Accuracy Score: {np.mean(accuracy_GBC)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.01 \t n_est: 50 \t depth: 1 \t accuracy: 0.749563953488372\n",
      "LR: 0.01 \t n_est: 50 \t depth: 3 \t accuracy: 0.7650678294573644\n",
      "LR: 0.01 \t n_est: 50 \t depth: 5 \t accuracy: 0.7953488372093023\n",
      "LR: 0.01 \t n_est: 100 \t depth: 1 \t accuracy: 0.7474806201550388\n",
      "LR: 0.01 \t n_est: 100 \t depth: 3 \t accuracy: 0.7871124031007752\n",
      "LR: 0.01 \t n_est: 100 \t depth: 5 \t accuracy: 0.8078488372093023\n",
      "LR: 0.01 \t n_est: 200 \t depth: 1 \t accuracy: 0.7624031007751938\n",
      "LR: 0.01 \t n_est: 200 \t depth: 3 \t accuracy: 0.8065406976744187\n",
      "LR: 0.01 \t n_est: 200 \t depth: 5 \t accuracy: 0.8163275193798448\n",
      "LR: 0.1 \t n_est: 50 \t depth: 1 \t accuracy: 0.7859011627906975\n",
      "LR: 0.1 \t n_est: 50 \t depth: 3 \t accuracy: 0.8268895348837211\n",
      "LR: 0.1 \t n_est: 50 \t depth: 5 \t accuracy: 0.8366279069767442\n",
      "LR: 0.1 \t n_est: 100 \t depth: 1 \t accuracy: 0.804360465116279\n",
      "LR: 0.1 \t n_est: 100 \t depth: 3 \t accuracy: 0.838953488372093\n",
      "LR: 0.1 \t n_est: 100 \t depth: 5 \t accuracy: 0.8421996124031008\n",
      "LR: 0.1 \t n_est: 200 \t depth: 1 \t accuracy: 0.8243701550387597\n",
      "LR: 0.1 \t n_est: 200 \t depth: 3 \t accuracy: 0.8425872093023257\n",
      "LR: 0.1 \t n_est: 200 \t depth: 5 \t accuracy: 0.844331395348837\n",
      "LR: 0.5 \t n_est: 50 \t depth: 1 \t accuracy: 0.8232558139534885\n",
      "LR: 0.5 \t n_est: 50 \t depth: 3 \t accuracy: 0.8435077519379846\n",
      "LR: 0.5 \t n_est: 50 \t depth: 5 \t accuracy: 0.8377422480620155\n",
      "LR: 0.5 \t n_est: 100 \t depth: 1 \t accuracy: 0.8343023255813954\n",
      "LR: 0.5 \t n_est: 100 \t depth: 3 \t accuracy: 0.8387112403100774\n",
      "LR: 0.5 \t n_est: 100 \t depth: 5 \t accuracy: 0.8347383720930234\n",
      "LR: 0.5 \t n_est: 200 \t depth: 1 \t accuracy: 0.8377906976744185\n",
      "LR: 0.5 \t n_est: 200 \t depth: 3 \t accuracy: 0.8387112403100776\n",
      "LR: 0.5 \t n_est: 200 \t depth: 5 \t accuracy: 0.8319767441860465\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cal = datasets.fetch_california_housing()\n",
    "X = cal['data']\n",
    "y = cal['target']\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "learning_rate = [0.01, 0.1, 0.5]\n",
    "n_estimators = [50, 100, 200]\n",
    "max_depth = [1, 3, 5]\n",
    "\n",
    "for l in learning_rate:\n",
    "    for n in n_estimators:\n",
    "        for m in max_depth:\n",
    "            accuracy_scores = []\n",
    "            GBC = GradientBoostingClassifier(learning_rate=l, n_estimators=n, max_depth=m)\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                GBC.fit(X_train, np.where(y_train > 2, 1, 0))\n",
    "                y_pred = GBC.predict(X_test)\n",
    "                accuracy_scores.append(accuracy_score(y_pred, np.where(y_test > 2, 1, 0)))\n",
    "            print(f'LR: {l} \\t n_est: {n} \\t depth: {m} \\t accuracy: {np.mean(accuracy_scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification ROC AUC Score: 0.7862541394388711\n",
      "GBTree Classification ROC AUC Score: 0.8396247238334131\n",
      "\n",
      "\n",
      "LR: 0.01 \t n_est: 50 \t depth: 1 \t ROC AUC Score: 0.7674944226613825\n",
      "LR: 0.01 \t n_est: 50 \t depth: 3 \t ROC AUC Score: 0.7943428033861013\n",
      "LR: 0.01 \t n_est: 50 \t depth: 5 \t ROC AUC Score: 0.8071434308801783\n",
      "LR: 0.01 \t n_est: 100 \t depth: 1 \t ROC AUC Score: 0.7626207905690827\n",
      "LR: 0.01 \t n_est: 100 \t depth: 3 \t ROC AUC Score: 0.7925264844254658\n",
      "LR: 0.01 \t n_est: 100 \t depth: 5 \t ROC AUC Score: 0.811116405455676\n",
      "LR: 0.01 \t n_est: 200 \t depth: 1 \t ROC AUC Score: 0.770935513988583\n",
      "LR: 0.01 \t n_est: 200 \t depth: 3 \t ROC AUC Score: 0.8057625831192674\n",
      "LR: 0.01 \t n_est: 200 \t depth: 5 \t ROC AUC Score: 0.8196742623761853\n",
      "LR: 0.1 \t n_est: 50 \t depth: 1 \t ROC AUC Score: 0.7908372815235094\n",
      "LR: 0.1 \t n_est: 50 \t depth: 3 \t ROC AUC Score: 0.8299240808897315\n",
      "LR: 0.1 \t n_est: 50 \t depth: 5 \t ROC AUC Score: 0.8374154761207245\n",
      "LR: 0.1 \t n_est: 100 \t depth: 1 \t ROC AUC Score: 0.8099027263005365\n",
      "LR: 0.1 \t n_est: 100 \t depth: 3 \t ROC AUC Score: 0.8395841894912822\n",
      "LR: 0.1 \t n_est: 100 \t depth: 5 \t ROC AUC Score: 0.8417581011852884\n",
      "LR: 0.1 \t n_est: 200 \t depth: 1 \t ROC AUC Score: 0.8275194898599546\n",
      "LR: 0.1 \t n_est: 200 \t depth: 3 \t ROC AUC Score: 0.843362067375994\n",
      "LR: 0.1 \t n_est: 200 \t depth: 5 \t ROC AUC Score: 0.841990771053586\n",
      "LR: 0.5 \t n_est: 50 \t depth: 1 \t ROC AUC Score: 0.8247026705242597\n",
      "LR: 0.5 \t n_est: 50 \t depth: 3 \t ROC AUC Score: 0.8377747557449666\n",
      "LR: 0.5 \t n_est: 50 \t depth: 5 \t ROC AUC Score: 0.8343662363183568\n",
      "LR: 0.5 \t n_est: 100 \t depth: 1 \t ROC AUC Score: 0.8369906641539886\n",
      "LR: 0.5 \t n_est: 100 \t depth: 3 \t ROC AUC Score: 0.8363449179307377\n",
      "LR: 0.5 \t n_est: 100 \t depth: 5 \t ROC AUC Score: 0.8308563275961479\n",
      "LR: 0.5 \t n_est: 200 \t depth: 1 \t ROC AUC Score: 0.8392080023100785\n",
      "LR: 0.5 \t n_est: 200 \t depth: 3 \t ROC AUC Score: 0.8344215515480908\n",
      "LR: 0.5 \t n_est: 200 \t depth: 5 \t ROC AUC Score: 0.8332924224580015\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "cal = datasets.fetch_california_housing()\n",
    "X = cal['data']\n",
    "y = cal['target']\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "roc_auc_score_log = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    log = LogisticRegression(solver='liblinear').fit(X_train, np.where(y_train > 2, 1, 0))\n",
    "    y_pred = log.predict(X_test)\n",
    "    roc_auc_score_log.append(roc_auc_score(y_pred, np.where(y_test > 2, 1, 0)))\n",
    "\n",
    "print(f'Logistic Regression Classification ROC AUC Score: {np.mean(roc_auc_score_log)}')\n",
    "\n",
    "roc_auc_score_GBC = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    GBC = GradientBoostingClassifier().fit(X_train, np.where(y_train > 2, 1, 0))\n",
    "    y_pred = GBC.predict(X_test)\n",
    "    roc_auc_score_GBC.append(roc_auc_score(y_pred, np.where(y_test > 2, 1, 0)))\n",
    "\n",
    "print(f'GBTree Classification ROC AUC Score: {np.mean(roc_auc_score_GBC)}')\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "cal = datasets.fetch_california_housing()\n",
    "X = cal['data']\n",
    "y = cal['target']\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "learning_rate = [0.01, 0.1, 0.5]\n",
    "n_estimators = [50, 100, 200]\n",
    "max_depth = [1, 3, 5]\n",
    "\n",
    "for l in learning_rate:\n",
    "    for n in n_estimators:\n",
    "        for m in max_depth:\n",
    "            roc_auc_scores = []\n",
    "            GBC = GradientBoostingClassifier(learning_rate=l, n_estimators=n, max_depth=m)\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                GBC.fit(X_train, np.where(y_train > 2, 1, 0))\n",
    "                y_pred = GBC.predict(X_test)\n",
    "                roc_auc_scores.append(roc_auc_score(y_pred, np.where(y_test > 2, 1, 0)))\n",
    "            print(f'LR: {l} \\t n_est: {n} \\t depth: {m} \\t ROC AUC Score: {np.mean(roc_auc_scores)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Classification ROC AUC Score: 0.5967975254794878\n",
      "KNN Classification Accuracy Score: 0.6017926356589147\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "cal = datasets.fetch_california_housing()\n",
    "X = cal['data']\n",
    "y = cal['target']\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "roc_auc_score_nb = []\n",
    "accuracy_score_nb = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    nb = KNeighborsClassifier().fit(X_train, np.where(y_train > 2, 1, 0))\n",
    "    y_pred = nb.predict(X_test)\n",
    "    roc_auc_score_nb.append(roc_auc_score(y_pred, np.where(y_test > 2, 1, 0)))\n",
    "    accuracy_score_nb.append(accuracy_score(y_pred, np.where(y_test > 2, 1, 0)))\n",
    "\n",
    "print(f'KNN Classification ROC AUC Score: {np.mean(roc_auc_score_nb)}')\n",
    "print(f'KNN Classification Accuracy Score: {np.mean(accuracy_score_nb)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance and Conclusions\n",
    "Based on part (a) and (c), the Gradient Boosting Tree Classification algorithm performs better than Logistic Regression Classification by both metrics, accuracy score and ROC score. Both classifiers perform significantly better than a trivial KNN classifier, also by both metrics. This suggests that the data has a linear relationship rather than a clustered or globular structure. Based on part (c), we see similar relationships between learning-rate, n estimators, depth and ROC score as we did with the regressors. However, the difference between learning-rates is not as large as we saw with regression, and the larger learning-rate does not suffer as much with the accuracy and ROC AUC metric, though a learning-rate of 0.1 remains highest among the three tested."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
